NAME
    gcloud alpha storage cp - upload, download, and copy Cloud Storage objects

SYNOPSIS
    gcloud alpha storage cp SOURCE [SOURCE ...] DESTINATION
        [--all-versions, -A] [--cache-control=CACHE_CONTROL]
        [--content-disposition=CONTENT_DISPOSITION]
        [--content-encoding=CONTENT_ENCODING]
        [--content-language=CONTENT_LANGUAGE] [--content-md5=MD5_DIGEST]
        [--content-type=CONTENT_TYPE] [--continue-on-error, -c]
        [--custom-metadata=[CUSTOM_METADATA,...]] [--custom-time=CUSTOM_TIME]
        [--do-not-decompress] [--if-generation-match=GENERATION]
        [--if-metageneration-match=METAGENERATION] [--ignore-symlinks]
        [--recursive, -R, -r] [--skip-unsupported, -U]
        [--storage-class=STORAGE_CLASS, -s STORAGE_CLASS]
        [--gzip-in-flight-all, -J
          | --gzip-in-flight-extensions=[FILE_EXTENSIONS,...],
          -j [FILE_EXTENSIONS,...]] [GCLOUD_WIDE_FLAG ...]

DESCRIPTION
    (ALPHA) Copy data between your local file system and the cloud, within the
    cloud, and between cloud storage providers.

EXAMPLES
    The following command uploads all text files from the local directory to a
    bucket:

        $ gcloud alpha storage cp *.txt gs://my-bucket

    The following command downloads all text files from a bucket to your
    current directory:

        $ gcloud alpha storage cp gs://my-bucket/*.txt .

    The following command transfers all text files from a bucket to a different
    cloud storage provider:

        $ gcloud alpha storage cp gs://my-bucket/*.txt s3://my-bucket

    Use the --recursive option to copy an entire directory tree. The following
    command uploads the directory tree dir:

        $ gcloud alpha storage cp --recursive dir gs://my-bucket

POSITIONAL ARGUMENTS
     SOURCE [SOURCE ...]
        The source path(s) to copy.

     DESTINATION
        The destination path.

FLAGS
     --all-versions, -A
        Copy all source versions from a source bucket or folder. If not set,
        only the live version of each source object is copied.

        Note: This option is only useful when the destination bucket has Object
        Versioning enabled. Additionally, the generation numbers of copied
        versions do not necessarily match the order of the original generation
        numbers.

     --cache-control=CACHE_CONTROL
        How caches should handle requests and responses.

     --content-disposition=CONTENT_DISPOSITION
        How content should be displayed.

     --content-encoding=CONTENT_ENCODING
        How content is encoded (e.g. gzip).

     --content-language=CONTENT_LANGUAGE
        Content's language (e.g. en signifies "English").

     --content-md5=MD5_DIGEST
        Manually specified MD5 hash digest for the contents of an uploaded
        file. This flag cannot be used when uploading multiple files. The
        custom digest is used by the cloud provider for validation.

     --content-type=CONTENT_TYPE
        Type of data contained in the object (e.g. text/html).

     --continue-on-error, -c
        If any operations are unsuccessful, the command will exit with a
        non-zero exit status after completing the remaining operations. This
        flag takes effect only in sequential execution mode (i.e. processor and
        thread count are set to 1). Parallelism is default.

     --custom-metadata=[CUSTOM_METADATA,...]
        Custom metadata fields set by user.

     --custom-time=CUSTOM_TIME
        Custom time for Google Cloud Storage objects in RFC 3339 format.

     --do-not-decompress
        Do not automatically decompress downloaded gzip files.

     --if-generation-match=GENERATION
        Execute only if the generation matches the generation of the requested
        object.

     --if-metageneration-match=METAGENERATION
        Execute only if the metageneration matches the metageneration of the
        requested object.

     --ignore-symlinks
        Ignore file symlinks instead of copying what they point to. Symlinks
        pointing to directories will always be ignored.

     --recursive, -R, -r
        Recursively copy the contents of any directories that match the source
        path expression.

     --skip-unsupported, -U
        Skip objects with unsupported object types.Currently, the only
        unsupported category is Amazon S3 objects with the GLACIER storage
        class.

     --storage-class=STORAGE_CLASS, -s STORAGE_CLASS
        Specifies the storage class of the destination object. If not
        specified, the default storage class of the destination bucket is used.
        This option is not valid for copying to non-cloud destinations.

     At most one of these can be specified:

       --gzip-in-flight-all, -J
          Applies gzip transport encoding to file uploads. This option works
          like the --gzip-transfer option described above, but it applies to
          all uploaded files, regardless of extension.

          CAUTION: If some of the source files don't compress well, such as
          binary data, using this option may result in longer uploads.

       --gzip-in-flight-extensions=[FILE_EXTENSIONS,...], -j [FILE_EXTENSIONS,...]
          Applies gzip transport encoding to any file upload whose extension
          matches the input extension list. This is useful when uploading files
          with compressible content such as .js, .css, or .html files. This
          also saves network bandwidth while leaving the data uncompressed in
          Cloud Storage.

          When you specify the --gzip-in-flight option, files being uploaded
          are compressed in-memory and on-the-wire only. Both the local files
          and Cloud Storage objects remain uncompressed. The uploaded objects
          retain the Content-Type and name of the original files.

GCLOUD WIDE FLAGS
    These flags are available to all commands: --access-token-file, --account,
    --billing-project, --configuration, --flags-file, --flatten, --format,
    --help, --impersonate-service-account, --log-http, --project, --quiet,
    --trace-token, --user-output-enabled, --verbosity.

    Run $ gcloud help for details.

NOTES
    This command is currently in alpha and might change without notice. If this
    command fails with API permission errors despite specifying the correct
    project, you might be trying to access an API with an invitation-only early
    access allowlist.

