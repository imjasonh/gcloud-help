NAME
    gcloud beta container node-pools update - updates a node pool in a running
        cluster

SYNOPSIS
    gcloud beta container node-pools update NAME
        (--enable-confidential-nodes | --enable-gvnic
          | --enable-image-streaming
          | --network-performance-configs=[PROPERTY=VALUE,...]
          | --node-labels=[NODE_LABEL,...] | --node-locations=ZONE,[ZONE,...]
          | --node-taints=[NODE_TAINT,...]
          | --system-config-from-file=SYSTEM_CONFIG_FROM_FILE
          | --tags=[TAG,...] | --workload-metadata=WORKLOAD_METADATA
          | --enable-autoprovisioning
          --enable-autoscaling --max-nodes=MAX_NODES --min-nodes=MIN_NODES
          | --enable-autorepair --enable-autoupgrade
          | --max-surge-upgrade=MAX_SURGE_UPGRADE
          --max-unavailable-upgrade=MAX_UNAVAILABLE_UPGRADE)
        [--cluster=CLUSTER] [--region=REGION | --zone=ZONE, -z ZONE]
        [GCLOUD_WIDE_FLAG ...]

DESCRIPTION
    (BETA) gcloud beta container node-pools update updates a node pool in a
    Google Kubernetes Engine cluster.

EXAMPLES
    To turn on node auto repair in "node-pool-1" in the cluster
    "sample-cluster", run:

        $ gcloud beta container node-pools update node-pool-1 \
            --cluster=sample-cluster --enable-autoupgrade

POSITIONAL ARGUMENTS
     NAME
        The name of the node pool.

REQUIRED FLAGS
     Exactly one of these must be specified:

       --enable-confidential-nodes
          Recreate all the nodes in the node pool to be confidential VM
          https://cloud.google.com/compute/confidential-vm/docs/about-cvm.

       --enable-gvnic
          Enable the use of GVNIC for this cluster. Requires re-creation of
          nodes using either a node-pool upgrade or node-pool creation.

       --enable-image-streaming
          Specifies whether to enable image streaming on node pool.

       --network-performance-configs=[PROPERTY=VALUE,...]
          Configures network performance settings for the node pool. If this
          flag is not specified, the pool will be created with its default
          network performance configuration.

           total-egress-bandwidth-tier
              Total egress bandwidth is the available outbound bandwidth from a
              VM, regardless of whether the traffic is going to internal IP or
              external IP destinations. The following tier values are allowed:
              [DEFAULT,TIER_1]

       --node-labels=[NODE_LABEL,...]
          Replaces all the user specified Kubernetes labels on all nodes in an
          existing node pool with the given labels.

          Examples:

              $ gcloud beta container node-pools update node-pool-1 \
                  --cluster=example-cluster \
                  --node-labels=label1=value1,label2=value2

          New nodes, including ones created by resize or recreate, will have
          these labels on the Kubernetes API node object and can be used in
          nodeSelectors. See
          http://kubernetes.io/docs/user-guide/node-selection/ for examples.

          Note that Kubernetes labels, intended to associate cluster components
          and resources with one another and manage resource lifecycles, are
          different from Google Kubernetes Engine labels that are used for the
          purpose of tracking billing and usage information.

       --node-locations=ZONE,[ZONE,...]
          Set of zones in which the node pool's nodes should be located.
          Changing the locations for a node pool will result in nodes being
          either created or removed from the node pool, depending on whether
          locations are being added or removed.

          Multiple locations can be specified, separated by commas. For
          example:

              $ gcloud beta container node-pools update node-pool-1 \
                  --cluster=sample-cluster \
                  --node-locations=us-central1-a,us-central1-b

       --node-taints=[NODE_TAINT,...]
          Replaces all the user specified Kubernetes taints on all nodes in an
          existing node pool, which can be used with tolerations for pod
          scheduling.

          Examples:

              $ gcloud beta container node-pools update node-pool-1 \
                  --cluster=example-cluster \
                  --node-taints=key1=val1:NoSchedule,key2=val2:PreferNoSchedule

          To read more about node-taints, see
          https://cloud.google.com/kubernetes-engine/docs/node-taints.

       --system-config-from-file=SYSTEM_CONFIG_FROM_FILE
          Path of the YAML/JSON file that contains the node configuration,
          including Linux kernel parameters (sysctls) and kubelet configs.

          Examples:

              kubeletConfig:
                cpuManagerPolicy: static
              linuxConfig:
                sysctl:
                  net.core.somaxconn: '2048'
                  net.ipv4.tcp_rmem: '4096 87380 6291456'

          List of supported kubelet configs in 'kubeletConfig'.

            KEY                VALUE
            cpuManagerPolicy   either 'static' or 'none'
            cpuCFSQuota        true or false (enabled by default)
            cpuCFSQuotaPeriod  interval (e.g., '100ms')

          List of supported sysctls in 'linuxConfig'.

            KEY                                         VALUE
            net.core.netdev_max_backlog                 Any positive integer,
                                                        less than 2147483647
            net.core.rmem_max                           Any positive integer,
                                                        less than 2147483647
            net.core.wmem_default                       Any positive integer,
                                                        less than 2147483647
            net.core.wmem_max                           Any positive integer,
                                                        less than 2147483647
            net.core.optmem_max                         Any positive integer,
                                                        less than 2147483647
            net.core.somaxconn                          Must be [128,
                                                        2147483647]
            net.ipv4.tcp_rmem                           Any positive integer
                                                        tuple
            net.ipv4.tcp_wmem                           Any positive integer
                                                        tuple
            net.ipv4.tcp_tw_reuse                       Must be {0, 1}

          Note, updating the system configuration of an existing node pool
          requires recreation of the nodes which which might cause a
          disruption.

       --tags=[TAG,...]
          Replaces all the user specified Compute Engine tags on all nodes in
          an existing node pool with the given tags (comma separated).

          Examples:

              $ gcloud beta container node-pools update node-pool-1 \
                  --cluster=example-cluster --tags=tag1,tag2

          New nodes, including ones created by resize or recreate, will have
          these tags on the Compute Engine API instance object and these tags
          can be used in firewall rules. See
          https://cloud.google.com/sdk/gcloud/reference/compute/firewall-rules/create
          for examples.

       --workload-metadata=WORKLOAD_METADATA
          Type of metadata server available to pods running in the node pool.
          WORKLOAD_METADATA must be one of:

           EXPOSED
              [DEPRECATED] Pods running in this node pool have access to the
              node's underlying Compute Engine Metadata Server.
           GCE_METADATA
              Pods running in this node pool have access to the node's
              underlying Compute Engine Metadata Server.
           GKE_METADATA
              Run the Kubernetes Engine Metadata Server on this node. The
              Kubernetes Engine Metadata Server exposes a metadata API to
              workloads that is compatible with the V1 Compute Metadata APIs
              exposed by the Compute Engine and App Engine Metadata Servers.
              This feature can only be enabled if Workload Identity is enabled
              at the cluster level.
           GKE_METADATA_SERVER
              [DEPRECATED] Run the Kubernetes Engine Metadata Server on this
              node. The Kubernetes Engine Metadata Server exposes a metadata
              API to workloads that is compatible with the V1 Compute Metadata
              APIs exposed by the Compute Engine and App Engine Metadata
              Servers. This feature can only be enabled if Workload Identity is
              enabled at the cluster level.
           SECURE
              [DEPRECATED] Prevents pods not in hostNetwork from accessing
              certain VM metadata, specifically kube-env, which contains
              Kubelet credentials, and the instance identity token. This is a
              temporary security solution available while the bootstrapping
              process for cluster nodes is being redesigned with significant
              security improvements. This feature is scheduled to be deprecated
              in the future and later removed.

       Cluster autoscaling

         --enable-autoprovisioning
            Enables Cluster Autoscaler to treat the node pool as if it was
            autoprovisioned.

            Cluster Autoscaler will be able to delete the node pool if it's
            unneeded.

         --enable-autoscaling
            Enables autoscaling for a node pool.

            Enables autoscaling in the node pool specified by --node-pool or
            the default node pool if --node-pool is not provided.

         --max-nodes=MAX_NODES
            Maximum number of nodes per zone in the node pool.

            Maximum number of nodes per zone to which the node pool specified
            by --node-pool (or default node pool if unspecified) can scale.
            Ignored unless --enable-autoscaling is also specified.

         --min-nodes=MIN_NODES
            Minimum number of nodes per zone in the node pool.

            Minimum number of nodes per zone to which the node pool specified
            by --node-pool (or default node pool if unspecified) can scale.
            Ignored unless --enable-autoscaling is also specified.

       Node management

         --enable-autorepair
            Enable node autorepair feature for a node pool.

                $ gcloud beta container node-pools update node-pool-1 \
                    --cluster=example-cluster --enable-autorepair

            See
            https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair
            for more info.

         --enable-autoupgrade
            Sets autoupgrade feature for a node pool.

                $ gcloud beta container node-pools update node-pool-1 \
                    --cluster=example-cluster --enable-autoupgrade

            See
            https://cloud.google.com/kubernetes-engine/docs/node-auto-upgrades
            for more info.

       Upgrade settings

         --max-surge-upgrade=MAX_SURGE_UPGRADE
            Number of extra (surge) nodes to be created on each upgrade of the
            node pool.

            Specifies the number of extra (surge) nodes to be created during
            this node pool's upgrades. For example, running the following
            command will result in creating an extra node each time the node
            pool is upgraded:

                $ gcloud beta container node-pools update node-pool-1 \
                    --cluster=example-cluster --max-surge-upgrade=1   \
                    --max-unavailable-upgrade=0

            Must be used in conjunction with '--max-unavailable-upgrade'.

         --max-unavailable-upgrade=MAX_UNAVAILABLE_UPGRADE
            Number of nodes that can be unavailable at the same time on each
            upgrade of the node pool.

            Specifies the number of nodes that can be unavailable at the same
            time during this node pool's upgrades. For example, assume the node
            pool has 5 nodes, running the following command will result in
            having 3 nodes being upgraded in parallel (1 + 2), but keeping
            always at least 3 (5 - 2) available each time the node pool is
            upgraded:

                $ gcloud beta container node-pools update node-pool-1 \
                    --cluster=example-cluster --max-surge-upgrade=1   \
                    --max-unavailable-upgrade=2

            Must be used in conjunction with '--max-surge-upgrade'.

OPTIONAL FLAGS
     --cluster=CLUSTER
        The name of the cluster. Overrides the default container/cluster
        property value for this command invocation.

     At most one of these can be specified:

       --region=REGION
          Compute region (e.g. us-central1) for the cluster.

       --zone=ZONE, -z ZONE
          Compute zone (e.g. us-central1-a) for the cluster. Overrides the
          default compute/zone property value for this command invocation.

GCLOUD WIDE FLAGS
    These flags are available to all commands: --access-token-file, --account,
    --billing-project, --configuration, --flags-file, --flatten, --format,
    --help, --impersonate-service-account, --log-http, --project, --quiet,
    --trace-token, --user-output-enabled, --verbosity.

    Run $ gcloud help for details.

NOTES
    This command is currently in beta and might change without notice. These
    variants are also available:

        $ gcloud container node-pools update

        $ gcloud alpha container node-pools update

